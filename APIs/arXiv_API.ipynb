{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Author** ::\n",
        "Muhammad Hassan Mukhtar\n",
        "\n",
        "**Affiliation** ::\n",
        "The University of Salford, Manchester, England, UK\n",
        "\n",
        "**Connect** ::\n",
        "[GitHub](https://github.com/MHM-Rajpoot)\n",
        "[LinkedIn](https://www.linkedin.com/in/-muhammad-hassan-mukhtar-/)"
      ],
      "metadata": {
        "id": "0LQxBPfDjXJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SetUp"
      ],
      "metadata": {
        "id": "i8N05Sn4My0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install datetime\n",
        "#!pip install xml.etree.ElementTree #Already in Base Pakages of Python\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyIzsL9V7bRG",
        "outputId": "3552f38f-c2c6-4101-d59d-159f2fb940ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime) (2025.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.1.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-7.2\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's essential to note that ignoring encoding warnings when writing data to a CSV file is generally not recommended. However, in this specific case, we're taking this approach because our subsequent data preprocessing steps will involve filtering out stop words and extraneous characters, effectively addressing potential encoding issues. This targeted approach ensures the quality and integrity of our dataset."
      ],
      "metadata": {
        "id": "FTw7xAwtfDrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "haocntRDe9j2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### arXiv API Data Acquisition Example"
      ],
      "metadata": {
        "id": "zssTBDCXS24F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching and Processing Research Papers from arXiv**\n",
        "\n",
        "This Colab notebook demonstrates how to interact with the arXiv API to fetch, download, and process research papers based on a search query.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "1. Retrieve Research Papers: Sends HTTP requests to the arXiv API, fetches XML data, and extracts relevant information such as titles, summaries, publication dates, and links.  \n",
        "2. Filter Recent Papers: Filters the fetched papers to include only those published in the last six months.  \n",
        "3. Download Full-Text PDFs: Constructs PDF URLs and downloads the research papers.  \n",
        "4. Extract and Save Text: Uses PyPDF2 to extract text from downloaded PDFs and saves the content in a CSV file with two columns: Paper ID and Raw Text.  \n",
        "5. Modular and Efficient: Implements functions for fetching, parsing, downloading, and saving papers, ensuring reusability and efficient batch processing.\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "- Define a search query, for example, \"machine learning.\"\n",
        "- Fetch and filter the latest research papers from arXiv.\n",
        "- Download the full-text PDFs.\n",
        "- Extract the text content and save it in a structured format.\n",
        "- Use the extracted data for further analysis or natural language processing tasks.\n",
        "\n",
        "This workflow automates research paper retrieval and processing, making it a valuable tool for academic research and data analysis."
      ],
      "metadata": {
        "id": "bMogiezdO4ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime, timedelta\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "RvTkjjhJY9O0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_articles(query, max_results=200, batch_size=10):\n",
        "    \"\"\"\n",
        "    Fetch research articles from the arXiv API based on a given search query.\n",
        "    Retrieves a specified number of articles in batches, sorted by submission date.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The keyword or topic to search for.\n",
        "        max_results (int): The maximum number of articles to retrieve.\n",
        "        batch_size (int): The number of articles per API request.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries with article details (title, summary, published date, and link).\n",
        "    \"\"\"\n",
        "    all_papers = []\n",
        "    for start in range(0, max_results, batch_size):\n",
        "        url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start={start}&max_results={batch_size}&sortBy=submittedDate&sortOrder=descending\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            papers = parse_arxiv_xml(response.text)\n",
        "            all_papers.extend(papers)\n",
        "        else:\n",
        "            print(f\"Failed to fetch papers at start={start}\")\n",
        "            break\n",
        "\n",
        "    return all_papers"
      ],
      "metadata": {
        "id": "u7DVaZ1kYMjt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arxiv_xml(xml_data):\n",
        "    \"\"\"\n",
        "    Parse XML response from the arXiv API and extract relevant article details.\n",
        "\n",
        "    Parameters:\n",
        "        xml_data (str): XML response from the arXiv API.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing article details (title, summary, published date, and link).\n",
        "    \"\"\"\n",
        "    root = ET.fromstring(xml_data)\n",
        "    papers = []\n",
        "\n",
        "    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
        "        title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
        "        summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
        "        published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
        "        link = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
        "        papers.append({\"title\": title, \"summary\": summary, \"published\": published, \"link\": link})\n",
        "\n",
        "    return papers"
      ],
      "metadata": {
        "id": "Z2kl4eCzW7oT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_paper(pdf_url, filename):\n",
        "    \"\"\"\n",
        "    Download and save a research paper from arXiv as a PDF.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_url (str): The URL to the PDF of the paper.\n",
        "        filename (str): The filename for saving the paper.\n",
        "    \"\"\"\n",
        "    response = requests.get(pdf_url, stream=True)\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Paper saved as {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download paper from {pdf_url}\")"
      ],
      "metadata": {
        "id": "L8pO1IM6ZNsB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a given PDF file.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text content from the PDF.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Zu3BPBJeZSoo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_papers_to_csv(papers):\n",
        "    \"\"\"\n",
        "    Saves extracted paper text into a CSV file with two columns: paper ID and raw text.\n",
        "\n",
        "    Parameters:\n",
        "        papers (list): List of dictionaries containing paper details.\n",
        "    \"\"\"\n",
        "    csv_filename = \"arxiv_papers.csv\"\n",
        "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Paper ID\", \"Raw Text\"])\n",
        "\n",
        "        for paper in papers:\n",
        "            arxiv_id = paper['link'].split('/')[-1]\n",
        "            pdf_filename = f\"./arxiv_papers/{arxiv_id}.pdf\"\n",
        "            text = extract_text_from_pdf(pdf_filename)\n",
        "            writer.writerow([arxiv_id, text.replace(\"\\ufffd\", \"?\")])  # Replace invalid characters with '?'\n",
        "\n",
        "    print(f\"Paper texts saved to {csv_filename}\")"
      ],
      "metadata": {
        "id": "2dsdyvMbZYH4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to fetch, filter, download, and process recent research papers from arXiv.\n",
        "\n",
        "    Workflow:\n",
        "    1. Fetches up to 200 research papers related to a specified query from the arXiv API.\n",
        "    2. Filters papers to include only those published within the last six months.\n",
        "    3. Downloads the full-text PDFs of the filtered papers.\n",
        "    4. Extracts text from the downloaded PDFs.\n",
        "    5. Saves the extracted text into a CSV file with two columns: Paper ID and Raw Text.\n",
        "\n",
        "    Steps:\n",
        "    - Defines a search query (default: \"machine learning\").\n",
        "    - Calls fetch_arxiv_articles() to retrieve papers.\n",
        "    - Filters results based on publication date.\n",
        "    - Iterates over the filtered papers to:\n",
        "      - Construct the arXiv ID and PDF URL.\n",
        "      - Download and save the paper using download_paper().\n",
        "    - Calls save_papers_to_csv() to store extracted text.\n",
        "\n",
        "    Outputs:\n",
        "    - Saves the downloaded PDFs in the \"./arxiv_papers/\" directory.\n",
        "    - Stores extracted text in a CSV file named \"arxiv_papers.csv\".\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    query = \"machine learning\"\n",
        "\n",
        "    # Choose your own max_results or leave default to get 200 results in batch of 10\n",
        "    papers = fetch_arxiv_articles(query, max_results=10, batch_size=1)\n",
        "\n",
        "    print(f\"\\nFetched {len(papers)} Papers\")\n",
        "\n",
        "    cutoff_date = datetime.now() - timedelta(days=180)  # 6 months ago\n",
        "    papers = [p for p in papers if datetime.strptime(p[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\") > cutoff_date]\n",
        "\n",
        "    print(f\"\\nFetched {len(papers)} Papers from the last 6 months\")\n",
        "\n",
        "    if not papers:\n",
        "        print(\"No recent papers found.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nDownloading and extracting text from papers:\")\n",
        "    break_ln = int(0)\n",
        "    for i, paper in enumerate(papers[:]):\n",
        "        arxiv_id = paper['link'].split('/')[-1]\n",
        "        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "        filename = f\"./arxiv_papers/{arxiv_id}.pdf\"\n",
        "        download_paper(pdf_url, filename)\n",
        "\n",
        "    print(\"\\nWriting Data to CSV\")\n",
        "    save_papers_to_csv(papers[:])\n",
        "    print(\"Process completed!\")"
      ],
      "metadata": {
        "id": "IBvik992ZeHb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFtpY95Uar5A",
        "outputId": "2ce89422-0d73-4e23-d6c1-1f17d967512c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetched 10 Papers\n",
            "\n",
            "Fetched 10 Papers from the last 6 months\n",
            "\n",
            "Downloading and extracting text from papers:\n",
            "Paper saved as ./arxiv_papers/2502.17437v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17436v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17433v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17432v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17429v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17427v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17425v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17424v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17423v1.pdf\n",
            "Paper saved as ./arxiv_papers/2502.17421v1.pdf\n",
            "\n",
            "Writing Data to CSV\n",
            "Paper texts saved to arxiv_papers.csv\n",
            "Process completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r arxiv_papers.zip arxiv_papers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp6jySXsbaLj",
        "outputId": "d0a8725e-7bf1-4684-e215-f22d457b793e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: arxiv_papers/ (stored 0%)\n",
            "  adding: arxiv_papers/2502.17427v1.pdf (deflated 11%)\n",
            "  adding: arxiv_papers/2502.17433v1.pdf (deflated 6%)\n",
            "  adding: arxiv_papers/2502.17436v1.pdf (deflated 9%)\n",
            "  adding: arxiv_papers/2502.17437v1.pdf (deflated 8%)\n",
            "  adding: arxiv_papers/2502.17421v1.pdf (deflated 7%)\n",
            "  adding: arxiv_papers/2502.17432v1.pdf (deflated 23%)\n",
            "  adding: arxiv_papers/2502.17423v1.pdf (deflated 1%)\n",
            "  adding: arxiv_papers/2502.17429v1.pdf (deflated 9%)\n",
            "  adding: arxiv_papers/2502.17425v1.pdf (deflated 10%)\n",
            "  adding: arxiv_papers/2502.17424v1.pdf (deflated 8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now possess a current and comprehensive dataset for tackling NLP challenges, comprising PDF and CSV files complete with filename and raw text extract features. This valuable resource is poised to fuel innovative project ideas and future endeavors.\n",
        "\n",
        "Thank you for your participation, and we conclude this tutorial here."
      ],
      "metadata": {
        "id": "9XqT38TkbjLP"
      }
    }
  ]
}